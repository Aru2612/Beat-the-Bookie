As part of this assignment we have also tried implementing deep neural networks for data analysis. We decisded to use framework named PyTorch.
Its a relatively new deep learnign framework that was made by facebook to compeete with TensorFlow. This framework allows easily to create neural networks both
linear and convoluted. Train them, test and save them as a file that can be loaded in the future.

First we have tried just traininga simple linear feed forward network by putting in the binary data of home and away teams, outputting 3 states, HomeWin, AwayWin and a Draw.
This network was very fast to train itself, however due to sparcity of the data it has failed to learn the relations between the teams.
Thus we have abondoned this particular architecture.
The architecture looked like:

----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                [-1, 1, 36]           2,628
            Linear-2                [-1, 1, 36]           1,332
            Linear-3                [-1, 1, 18]             666
            Linear-4                [-1, 1, 18]             342
            Linear-5                 [-1, 1, 9]             171
            Linear-6                 [-1, 1, 3]              30
================================================================
Total params: 5,169
Trainable params: 5,169
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.02
Estimated Total Size (MB): 0.02
----------------------------------------------------------------


We have also implemented a deep convolutional network using PyTorch. This allowed the fetures to be interprited together,
and the ability to find corelations between them. In all of the layers we have used a a kernel of size 5, thus in the first and the seccond layer we have reduced the size of the layers
each time. We have however then used padding of 1 to keep the size of layers the same. In the end we have just added a linear layer to create the 3 outputs of HomeTeam Win, AwayTeam Win and Draw
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv1d-1                [-1, 1, 15]               6
            Conv1d-2                [-1, 1, 11]               6
            Conv1d-3                [-1, 1, 11]               6
            Conv1d-4                [-1, 1, 11]               6
            Conv1d-5                [-1, 1, 11]               6
            Linear-6                 [-1, 1, 3]              36
================================================================
Total params: 66
Trainable params: 66
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.00
Estimated Total Size (MB): 0.00
----------------------------------------------------------------

This approach has significatly decreased the number of parameters that our model will have to learn, and also it would be able to closer relate some charectirisctics with each other.
Both architectures however have failed to produce good results, giving us nearly random guesses out the outputs and success rate of roughly 33 percent. We were unable to exactly explain why this was the case
however we do have a couple of ideas. First being the lack of a large enough data sample for each team. And the other being the sparcity of the data that was fead into the network.
